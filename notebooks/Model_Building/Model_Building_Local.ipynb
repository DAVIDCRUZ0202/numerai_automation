{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numerapi -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numerapi as nai\n",
    "from zipfile import ZipFile\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import r2_score\n",
    "import re\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locally, we don't need to ping the API to pull data for every kernel reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('numerai_training_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>era</th>\n",
       "      <th>data_type</th>\n",
       "      <th>feature_intelligence1</th>\n",
       "      <th>feature_intelligence2</th>\n",
       "      <th>feature_intelligence3</th>\n",
       "      <th>feature_intelligence4</th>\n",
       "      <th>feature_intelligence5</th>\n",
       "      <th>feature_intelligence6</th>\n",
       "      <th>feature_intelligence7</th>\n",
       "      <th>feature_intelligence8</th>\n",
       "      <th>feature_intelligence9</th>\n",
       "      <th>feature_intelligence10</th>\n",
       "      <th>feature_intelligence11</th>\n",
       "      <th>feature_intelligence12</th>\n",
       "      <th>feature_charisma1</th>\n",
       "      <th>feature_charisma2</th>\n",
       "      <th>feature_charisma3</th>\n",
       "      <th>feature_charisma4</th>\n",
       "      <th>feature_charisma5</th>\n",
       "      <th>feature_charisma6</th>\n",
       "      <th>feature_charisma7</th>\n",
       "      <th>feature_charisma8</th>\n",
       "      <th>feature_charisma9</th>\n",
       "      <th>feature_charisma10</th>\n",
       "      <th>feature_charisma11</th>\n",
       "      <th>feature_charisma12</th>\n",
       "      <th>feature_charisma13</th>\n",
       "      <th>feature_charisma14</th>\n",
       "      <th>feature_charisma15</th>\n",
       "      <th>feature_charisma16</th>\n",
       "      <th>feature_charisma17</th>\n",
       "      <th>feature_charisma18</th>\n",
       "      <th>feature_charisma19</th>\n",
       "      <th>feature_charisma20</th>\n",
       "      <th>feature_charisma21</th>\n",
       "      <th>feature_charisma22</th>\n",
       "      <th>feature_charisma23</th>\n",
       "      <th>feature_charisma24</th>\n",
       "      <th>feature_charisma25</th>\n",
       "      <th>feature_charisma26</th>\n",
       "      <th>feature_charisma27</th>\n",
       "      <th>feature_charisma28</th>\n",
       "      <th>feature_charisma29</th>\n",
       "      <th>feature_charisma30</th>\n",
       "      <th>feature_charisma31</th>\n",
       "      <th>feature_charisma32</th>\n",
       "      <th>feature_charisma33</th>\n",
       "      <th>feature_charisma34</th>\n",
       "      <th>feature_charisma35</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_constitution112</th>\n",
       "      <th>feature_constitution113</th>\n",
       "      <th>feature_constitution114</th>\n",
       "      <th>feature_wisdom1</th>\n",
       "      <th>feature_wisdom2</th>\n",
       "      <th>feature_wisdom3</th>\n",
       "      <th>feature_wisdom4</th>\n",
       "      <th>feature_wisdom5</th>\n",
       "      <th>feature_wisdom6</th>\n",
       "      <th>feature_wisdom7</th>\n",
       "      <th>feature_wisdom8</th>\n",
       "      <th>feature_wisdom9</th>\n",
       "      <th>feature_wisdom10</th>\n",
       "      <th>feature_wisdom11</th>\n",
       "      <th>feature_wisdom12</th>\n",
       "      <th>feature_wisdom13</th>\n",
       "      <th>feature_wisdom14</th>\n",
       "      <th>feature_wisdom15</th>\n",
       "      <th>feature_wisdom16</th>\n",
       "      <th>feature_wisdom17</th>\n",
       "      <th>feature_wisdom18</th>\n",
       "      <th>feature_wisdom19</th>\n",
       "      <th>feature_wisdom20</th>\n",
       "      <th>feature_wisdom21</th>\n",
       "      <th>feature_wisdom22</th>\n",
       "      <th>feature_wisdom23</th>\n",
       "      <th>feature_wisdom24</th>\n",
       "      <th>feature_wisdom25</th>\n",
       "      <th>feature_wisdom26</th>\n",
       "      <th>feature_wisdom27</th>\n",
       "      <th>feature_wisdom28</th>\n",
       "      <th>feature_wisdom29</th>\n",
       "      <th>feature_wisdom30</th>\n",
       "      <th>feature_wisdom31</th>\n",
       "      <th>feature_wisdom32</th>\n",
       "      <th>feature_wisdom33</th>\n",
       "      <th>feature_wisdom34</th>\n",
       "      <th>feature_wisdom35</th>\n",
       "      <th>feature_wisdom36</th>\n",
       "      <th>feature_wisdom37</th>\n",
       "      <th>feature_wisdom38</th>\n",
       "      <th>feature_wisdom39</th>\n",
       "      <th>feature_wisdom40</th>\n",
       "      <th>feature_wisdom41</th>\n",
       "      <th>feature_wisdom42</th>\n",
       "      <th>feature_wisdom43</th>\n",
       "      <th>feature_wisdom44</th>\n",
       "      <th>feature_wisdom45</th>\n",
       "      <th>feature_wisdom46</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n000315175b67977</td>\n",
       "      <td>era1</td>\n",
       "      <td>train</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n0014af834a96cdd</td>\n",
       "      <td>era1</td>\n",
       "      <td>train</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n001c93979ac41d4</td>\n",
       "      <td>era1</td>\n",
       "      <td>train</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n0034e4143f22a13</td>\n",
       "      <td>era1</td>\n",
       "      <td>train</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n00679d1a636062f</td>\n",
       "      <td>era1</td>\n",
       "      <td>train</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 314 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id   era data_type  feature_intelligence1  \\\n",
       "0  n000315175b67977  era1     train                   0.00   \n",
       "1  n0014af834a96cdd  era1     train                   0.00   \n",
       "2  n001c93979ac41d4  era1     train                   0.25   \n",
       "3  n0034e4143f22a13  era1     train                   1.00   \n",
       "4  n00679d1a636062f  era1     train                   0.25   \n",
       "\n",
       "   feature_intelligence2  feature_intelligence3  feature_intelligence4  \\\n",
       "0                   0.50                   0.25                   0.00   \n",
       "1                   0.00                   0.00                   0.25   \n",
       "2                   0.50                   0.25                   0.25   \n",
       "3                   0.00                   0.00                   0.50   \n",
       "4                   0.25                   0.25                   0.25   \n",
       "\n",
       "   feature_intelligence5  feature_intelligence6  feature_intelligence7  \\\n",
       "0                    0.5                   0.25                   0.25   \n",
       "1                    0.5                   0.00                   0.00   \n",
       "2                    1.0                   0.75                   0.75   \n",
       "3                    0.5                   0.25                   0.25   \n",
       "4                    0.0                   0.25                   0.50   \n",
       "\n",
       "   feature_intelligence8  feature_intelligence9  feature_intelligence10  \\\n",
       "0                   0.25                   0.75                    0.75   \n",
       "1                   0.25                   0.50                    0.50   \n",
       "2                   0.25                   0.00                    0.25   \n",
       "3                   0.75                   0.25                    0.50   \n",
       "4                   0.25                   0.25                    0.50   \n",
       "\n",
       "   feature_intelligence11  feature_intelligence12  feature_charisma1  \\\n",
       "0                    0.25                    0.25               1.00   \n",
       "1                    0.00                    0.50               0.00   \n",
       "2                    0.50                    1.00               0.50   \n",
       "3                    0.50                    0.50               0.75   \n",
       "4                    0.25                    0.25               0.75   \n",
       "\n",
       "   feature_charisma2  feature_charisma3  feature_charisma4  feature_charisma5  \\\n",
       "0               0.75                0.5                1.0                0.5   \n",
       "1               0.50                0.5                0.5                0.5   \n",
       "2               0.75                0.5                0.5                1.0   \n",
       "3               0.50                1.0                0.5                0.5   \n",
       "4               0.50                0.0                0.5                0.5   \n",
       "\n",
       "   feature_charisma6  feature_charisma7  feature_charisma8  feature_charisma9  \\\n",
       "0               0.00               0.50               0.50               0.00   \n",
       "1               0.50               0.25               0.25               0.50   \n",
       "2               0.50               0.50               0.50               0.25   \n",
       "3               0.00               1.00               0.00               0.75   \n",
       "4               0.25               0.00               0.50               0.00   \n",
       "\n",
       "   feature_charisma10  feature_charisma11  feature_charisma12  \\\n",
       "0                 0.0                0.00                1.00   \n",
       "1                 0.0                1.00                0.50   \n",
       "2                 0.0                0.25                0.75   \n",
       "3                 0.0                0.50                0.50   \n",
       "4                 0.5                0.25                0.50   \n",
       "\n",
       "   feature_charisma13  feature_charisma14  feature_charisma15  \\\n",
       "0                0.25                0.00                0.50   \n",
       "1                0.50                0.50                0.75   \n",
       "2                0.75                0.75                0.50   \n",
       "3                0.50                0.50                0.00   \n",
       "4                0.50                0.50                0.50   \n",
       "\n",
       "   feature_charisma16  feature_charisma17  feature_charisma18  \\\n",
       "0                0.25                0.75                0.50   \n",
       "1                0.50                0.50                0.75   \n",
       "2                0.75                0.50                0.25   \n",
       "3                0.50                0.50                0.75   \n",
       "4                0.50                0.50                0.50   \n",
       "\n",
       "   feature_charisma19  feature_charisma20  feature_charisma21  \\\n",
       "0                1.00                0.75                0.75   \n",
       "1                0.25                0.50                0.75   \n",
       "2                0.50                0.75                0.25   \n",
       "3                0.75                0.50                0.25   \n",
       "4                0.75                0.50                0.25   \n",
       "\n",
       "   feature_charisma22  feature_charisma23  feature_charisma24  \\\n",
       "0                 0.5                0.50                0.75   \n",
       "1                 0.5                0.25                0.75   \n",
       "2                 0.5                0.50                0.75   \n",
       "3                 0.5                0.50                0.50   \n",
       "4                 0.5                0.50                0.50   \n",
       "\n",
       "   feature_charisma25  feature_charisma26  feature_charisma27  \\\n",
       "0                 0.5                0.25                0.75   \n",
       "1                 0.5                0.50                0.50   \n",
       "2                 0.5                0.50                0.25   \n",
       "3                 0.5                0.50                0.50   \n",
       "4                 0.5                0.25                0.50   \n",
       "\n",
       "   feature_charisma28  feature_charisma29  feature_charisma30  \\\n",
       "0                0.75                0.50                0.25   \n",
       "1                0.75                0.50                0.50   \n",
       "2                0.75                0.75                0.50   \n",
       "3                0.50                0.75                0.50   \n",
       "4                0.25                0.00                0.50   \n",
       "\n",
       "   feature_charisma31  feature_charisma32  feature_charisma33  \\\n",
       "0                0.25                0.50                0.25   \n",
       "1                0.75                0.25                0.50   \n",
       "2                0.75                0.00                0.75   \n",
       "3                0.50                1.00                0.50   \n",
       "4                0.75                0.25                0.50   \n",
       "\n",
       "   feature_charisma34  feature_charisma35  ...  feature_constitution112  \\\n",
       "0                0.50                0.25  ...                     0.50   \n",
       "1                0.25                0.50  ...                     0.00   \n",
       "2                0.50                0.75  ...                     0.00   \n",
       "3                0.50                0.50  ...                     0.75   \n",
       "4                0.50                0.50  ...                     0.75   \n",
       "\n",
       "   feature_constitution113  feature_constitution114  feature_wisdom1  \\\n",
       "0                     0.50                     0.25             0.25   \n",
       "1                     1.00                     0.75             0.50   \n",
       "2                     0.25                     0.00             1.00   \n",
       "3                     0.25                     0.25             1.00   \n",
       "4                     0.50                     0.50             0.25   \n",
       "\n",
       "   feature_wisdom2  feature_wisdom3  feature_wisdom4  feature_wisdom5  \\\n",
       "0             1.00             0.75             0.50             0.75   \n",
       "1             1.00             0.00             0.25             0.00   \n",
       "2             0.50             1.00             0.75             0.00   \n",
       "3             1.00             1.00             1.00             0.75   \n",
       "4             0.75             0.00             0.50             0.50   \n",
       "\n",
       "   feature_wisdom6  feature_wisdom7  feature_wisdom8  feature_wisdom9  \\\n",
       "0             0.75             0.75             0.75             0.75   \n",
       "1             1.00             1.00             0.25             0.25   \n",
       "2             1.00             0.75             0.25             1.00   \n",
       "3             0.75             1.00             1.00             1.00   \n",
       "4             1.00             0.75             1.00             0.25   \n",
       "\n",
       "   feature_wisdom10  feature_wisdom11  feature_wisdom12  feature_wisdom13  \\\n",
       "0              0.75              0.50              1.00              1.00   \n",
       "1              0.25              1.00              1.00              0.50   \n",
       "2              1.00              1.00              0.50              1.00   \n",
       "3              0.75              0.75              1.00              0.75   \n",
       "4              0.75              1.00              0.75              0.00   \n",
       "\n",
       "   feature_wisdom14  feature_wisdom15  feature_wisdom16  feature_wisdom17  \\\n",
       "0              0.50              0.75               0.5              0.25   \n",
       "1              0.50              0.50               0.0              0.25   \n",
       "2              1.00              1.00               0.5              1.00   \n",
       "3              0.75              0.75               1.0              0.75   \n",
       "4              0.00              0.75               0.5              1.00   \n",
       "\n",
       "   feature_wisdom18  feature_wisdom19  feature_wisdom20  feature_wisdom21  \\\n",
       "0              0.25              0.75              0.50              1.00   \n",
       "1              1.00              0.50              1.00              1.00   \n",
       "2              0.00              1.00              1.00              0.50   \n",
       "3              1.00              0.75              1.00              0.75   \n",
       "4              0.50              0.75              0.25              0.50   \n",
       "\n",
       "   feature_wisdom22  feature_wisdom23  feature_wisdom24  feature_wisdom25  \\\n",
       "0               0.5              0.75              0.75              0.25   \n",
       "1               0.5              0.50              0.50              1.00   \n",
       "2               1.0              0.75              1.00              0.00   \n",
       "3               1.0              0.00              0.50              0.75   \n",
       "4               0.0              0.50              0.50              0.50   \n",
       "\n",
       "   feature_wisdom26  feature_wisdom27  feature_wisdom28  feature_wisdom29  \\\n",
       "0              0.50              1.00              0.75              0.50   \n",
       "1              0.25              0.75              1.00              0.25   \n",
       "2              0.50              0.75              0.00              1.00   \n",
       "3              1.00              0.75              1.00              0.75   \n",
       "4              0.75              0.75              0.50              0.75   \n",
       "\n",
       "   feature_wisdom30  feature_wisdom31  feature_wisdom32  feature_wisdom33  \\\n",
       "0              0.50              1.00              0.25               0.5   \n",
       "1              0.25              1.00              0.50               0.5   \n",
       "2              0.50              0.50              0.75               1.0   \n",
       "3              1.00              1.00              0.00               0.5   \n",
       "4              0.25              0.75              0.50               0.5   \n",
       "\n",
       "   feature_wisdom34  feature_wisdom35  feature_wisdom36  feature_wisdom37  \\\n",
       "0              0.50              0.50              0.75              1.00   \n",
       "1              0.50              0.75              0.75              0.75   \n",
       "2              0.75              1.00              0.25              0.50   \n",
       "3              0.75              0.75              1.00              0.75   \n",
       "4              0.25              0.25              0.75              0.50   \n",
       "\n",
       "   feature_wisdom38  feature_wisdom39  feature_wisdom40  feature_wisdom41  \\\n",
       "0              1.00              1.00              0.75              0.50   \n",
       "1              1.00              1.00              0.00              0.00   \n",
       "2              0.25              0.50              0.00              0.00   \n",
       "3              1.00              1.00              0.75              0.75   \n",
       "4              0.75              0.75              0.25              0.50   \n",
       "\n",
       "   feature_wisdom42  feature_wisdom43  feature_wisdom44  feature_wisdom45  \\\n",
       "0              0.75              0.50              1.00              0.50   \n",
       "1              0.75              0.25              0.00              0.25   \n",
       "2              0.50              1.00              0.00              0.25   \n",
       "3              1.00              1.00              0.75              1.00   \n",
       "4              0.75              0.00              0.50              0.25   \n",
       "\n",
       "   feature_wisdom46  target  \n",
       "0              0.75    0.50  \n",
       "1              1.00    0.25  \n",
       "2              0.75    0.25  \n",
       "3              1.00    0.25  \n",
       "4              0.75    0.75  \n",
       "\n",
       "[5 rows x 314 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    clean_df = df.copy()\n",
    "    featnames = [f for f in df if f.startswith(\"feature\")]\n",
    "# model training found in the examples do not \n",
    "# include the 'era' feature\n",
    "#     featnames.append('era')\n",
    "#     clean_df['era'] = df['era'].str.slice(3).astype(int)\n",
    "    X = clean_df[featnames]\n",
    "    y = clean_df['target']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size = 0.3, random_state=5)\n",
    "        \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = clean_data(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(LinearRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('linearregression', LinearRegression())])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerai has some competition specific scoring methods. We have rank-correlation and regular correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spearmans correlation btwn predictions and target\n",
    "def numerai_score(y_true, y_pred):\n",
    "    return spearmanr(y_true, y_pred)\n",
    "\n",
    "\n",
    "def correlation_score(y_true, y_pred):\n",
    "    return np.corrcoef(y_true, y_pred)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 Score: 0.0019004237644421051\n",
      "Normal Correlation: 0.044545018409113075\n",
      "(Numerai Score): SpearmanrResult(correlation=0.042787128962431975, pvalue=6.016634910876282e-62)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "score = r2_score(y_test, preds)\n",
    "print(f\"r2 Score: {score}\")\n",
    "print(f\"Normal Correlation: {correlation_score(y_test, preds)}\")\n",
    "print(f\"(Numerai Score): {numerai_score(y_test, preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Models/pipe.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(pipe, 'Models/pipe.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Exploration\n",
    "\n",
    "Predicting on the entire dataset returns not very impressive results at all. This data is cleaned and obfuscated, so engineering features from this data will be a result of examining machine results and working backwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Search\n",
    "\n",
    "We will implement various machine learning algorithms to get an idea of which ones perform at which levels. We will choose the best algorithms and then expand on this search in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(model, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Internal use for quick_and_dirty\n",
    "    \n",
    "    Scores a model with a dataset and\n",
    "    returns metrics\n",
    "    \n",
    "    args:\n",
    "    model - ml model to score\n",
    "    df - dataset to use in scoring\n",
    "    \n",
    "    returns:\n",
    "    model performance metrics\n",
    "    \"\"\"\n",
    "    print(\"Fitting Model Now\")\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Predicting with Model Now\")\n",
    "    preds = model.predict(X_test)\n",
    "    score = r2_score(y_test, preds)\n",
    "    print(f\"r2 Score: {score}\")\n",
    "    print(f\"Normal Correlation: {correlation_score(y_test, preds)}\")\n",
    "    print(f\"(Numerai Score): {numerai_score(y_test, preds)}\")\n",
    "    \n",
    "def quick_model(df):\n",
    "    \"\"\"\n",
    "    find promising model\n",
    "    \n",
    "    Trains a broad selection of different\n",
    "    regression algorithms from the sklearn\n",
    "    library. This function is meant to be\n",
    "    used as a first-pass at selecting which\n",
    "    models are best suited to solves a\n",
    "    regression problem.\n",
    "    \n",
    "    args:\n",
    "    df - the dataframe to perform ML on\n",
    "    \n",
    "    returns:\n",
    "    output of model performances\n",
    "    \"\"\"\n",
    "    from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "    from sklearn.linear_model import LinearRegression, Ridge, SGDRegressor, ElasticNet, BayesianRidge, PoissonRegressor\n",
    "    gbr = GradientBoostingRegressor()\n",
    "    rfr = RandomForestRegressor(max_samples=0.1)\n",
    "    lr = LinearRegression()\n",
    "    r = Ridge()\n",
    "    sgdr = SGDRegressor()\n",
    "    en = ElasticNet()\n",
    "    br = BayesianRidge()\n",
    "    pr = PoissonRegressor()\n",
    "    models = [gbr, rfr, lr, r, sgdr, en, br, pr]\n",
    "    X_train, X_test, y_train, y_test = clean_data(df)\n",
    "    for mod in models:\n",
    "        print(mod)\n",
    "        score(mod, X_train=X_train, X_test=X_test,\n",
    "              y_train=y_train, y_test=y_test)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingRegressor()\n",
      "Fitting Model Now\n",
      "Predicting with Model Now\n",
      "r2 Score: 0.0022196532238080158\n",
      "Normal Correlation: 0.047724737923815985\n",
      "(Numerai Score): SpearmanrResult(correlation=0.046248777603705944, pvalue=4.4716572949495604e-72)\n",
      "RandomForestRegressor(max_samples=0.1)\n",
      "Fitting Model Now\n",
      "Predicting with Model Now\n",
      "r2 Score: -0.01038649860242069\n",
      "Normal Correlation: 0.014390017644607284\n",
      "(Numerai Score): SpearmanrResult(correlation=0.013048165084119594, pvalue=4.130463049044258e-07)\n",
      "LinearRegression()\n",
      "Fitting Model Now\n",
      "Predicting with Model Now\n",
      "r2 Score: 0.0019004237644421051\n",
      "Normal Correlation: 0.044545018409113075\n",
      "(Numerai Score): SpearmanrResult(correlation=0.042787128962431975, pvalue=6.016634910876282e-62)\n",
      "Ridge()\n",
      "Fitting Model Now\n",
      "Predicting with Model Now\n",
      "r2 Score: 0.0019006959059808537\n",
      "Normal Correlation: 0.044546847332916834\n",
      "(Numerai Score): SpearmanrResult(correlation=0.04278905261660443, pvalue=5.942142751138534e-62)\n",
      "SGDRegressor()\n",
      "Fitting Model Now\n",
      "Predicting with Model Now\n",
      "r2 Score: -0.020706346416156318\n",
      "Normal Correlation: 0.02306635560366788\n",
      "(Numerai Score): SpearmanrResult(correlation=0.021967730501406366, pvalue=1.53592101003266e-17)\n",
      "ElasticNet()\n",
      "Fitting Model Now\n",
      "Predicting with Model Now\n",
      "r2 Score: -1.7808516133310803e-05\n",
      "Normal Correlation: -2.7100824068951163e-16\n",
      "(Numerai Score): SpearmanrResult(correlation=nan, pvalue=nan)\n",
      "BayesianRidge()\n",
      "Fitting Model Now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidcruz/miniconda3/envs/ml/lib/python3.9/site-packages/scipy/stats/stats.py:4484: SpearmanRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(SpearmanRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting with Model Now\n",
      "r2 Score: 0.0019460473251393662\n",
      "Normal Correlation: 0.044664654908278305\n",
      "(Numerai Score): SpearmanrResult(correlation=0.04400317293294172, pvalue=2.044905903672849e-65)\n",
      "PoissonRegressor()\n",
      "Fitting Model Now\n",
      "Predicting with Model Now\n",
      "r2 Score: 0.0005353459126146154\n",
      "Normal Correlation: 0.03391190850531068\n",
      "(Numerai Score): SpearmanrResult(correlation=0.03274200787610012, pvalue=5.396002117185694e-37)\n"
     ]
    }
   ],
   "source": [
    "quick_model(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "Notice that throughout results, we will continuously discuss r2 score and many of them will be negative. [This post](https://stats.stackexchange.com/questions/12900/when-is-r-squared-negative) helps us orient ourselves in what these scores mean. I am using this score at this point as an indicator that the model is capable of learning anything from the data.\n",
    "\n",
    "Based on the above results, RandomForestRegressor shows to be one of our best candidates. Although the correlations produced are not the best out of the bunch, that can be optimized later on. For the Spearman results, the produced p-value is much closer to a realistic value that can be improved upon. Also, with r2-score of -.01, this is the strongest indicating signal that the model is predicting SOMETHING rather than noise. We can see the r2-score of the rest of the models fall into ranges of .00xxx, meaning that those models are closer to being defined as a model which explains none of the variance between the response variable and its mean.\n",
    "\n",
    "A second interesting model is the gradient boosting regressor, which beats the rest of the models in terms of r2 score just slightly. Gradient boosting is a broad technique in ML and we can work with different algorithms which implement gradient boosting to see if we can expand on this slight edge in performance.\n",
    "\n",
    "A third algorithm with notable performance is the SGDregressor. This model came in second in terms of confidence with the spearman correlation coefficient, which means that it is much closer to being a realistic indicator of performance on unseen data. SGD is also a great idea to expand upon, because it introduces randomness into the method for optimizing the model.\n",
    "\n",
    "We should run a couple of more 'bare-bones' models to make sure that we choose the best. To continue this approach, I'll edit the above function to take a different list of models, and run through the training data one more time. We will build off of our observations above and focus on random forests and boosted models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changes Made\n",
    "\n",
    "Considering the models chosen above, we will now take steps in the (hopefully) right general direction by applying more advanced algorithms and ideas to our searching.\n",
    "\n",
    "We will enhance gradient boosting by using the xtreme gradient boosting model, an adaptation of the gradient boosting regressor which utilizes parallel tree boosting to seriously increase efficiency. \n",
    "\n",
    "We will also enhance the capabilities of our random forest regressor,by raising the number of samples used to build each tree by 0.1. This will make the algorithm take longer to execute, but we should see a difference in results.\n",
    "\n",
    "We will also experiment with Adaboost, another adaptation of gradient boosting which wraps an estimator in a gradient boosting object. Without setting parameters, we will see the performance of a decision tree regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_model(df):\n",
    "    \"\"\"\n",
    "    find promising model\n",
    "    \n",
    "    Trains a broad selection of different\n",
    "    regression algorithms from the sklearn\n",
    "    library. This function is meant to be\n",
    "    used as a first-pass at selecting which\n",
    "    models are best suited to solves a\n",
    "    regression problem.\n",
    "    \n",
    "    args:\n",
    "    df - the dataframe to perform ML on\n",
    "    \n",
    "    returns:\n",
    "    output of model performances\n",
    "    \"\"\"\n",
    "    from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
    "    from sklearn.linear_model import SGDRegressor\n",
    "    from xgboost import XGBRegressor\n",
    "    abr = AdaBoostRegressor()\n",
    "    xgb = XGBRegressor()\n",
    "    gbr = GradientBoostingRegressor()\n",
    "    rfr = RandomForestRegressor(max_samples=0.2)\n",
    "    sgdr = SGDRegressor(penalty=None)\n",
    "    models = [abr, xgb, gbr, rfr, sgdr]\n",
    "    X_train, X_test, y_train, y_test = clean_data(df)\n",
    "    for mod in models:\n",
    "        print(mod)\n",
    "        score(mod, X_train=X_train, X_test=X_test,\n",
    "              y_train=y_train, y_test=y_test)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostRegressor()\n",
      "Fitting Model Now\n",
      "Predicting with Model Now\n",
      "r2 Score: 0.00041859831320167906\n",
      "Normal Correlation: 0.021127765395844877\n",
      "(Numerai Score): SpearmanrResult(correlation=0.019751537167521056, pvalue=1.7987231835069024e-14)\n",
      "XGBRegressor(base_score=None, booster=None, colsample_bylevel=None,\n",
      "             colsample_bynode=None, colsample_bytree=None,\n",
      "             enable_categorical=False, gamma=None, gpu_id=None,\n",
      "             importance_type=None, interaction_constraints=None,\n",
      "             learning_rate=None, max_delta_step=None, max_depth=None,\n",
      "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, reg_alpha=None, reg_lambda=None,\n",
      "             scale_pos_weight=None, subsample=None, tree_method=None,\n",
      "             validate_parameters=None, verbosity=None)\n",
      "Fitting Model Now\n",
      "Predicting with Model Now\n",
      "r2 Score: -0.01162828425422413\n",
      "Normal Correlation: 0.035670929790722616\n",
      "(Numerai Score): SpearmanrResult(correlation=0.03464760825883031, pvalue=3.1995645390403606e-41)\n",
      "GradientBoostingRegressor()\n",
      "Fitting Model Now\n",
      "Predicting with Model Now\n",
      "r2 Score: 0.0022196532238080158\n",
      "Normal Correlation: 0.047724737923815985\n",
      "(Numerai Score): SpearmanrResult(correlation=0.046248777603705944, pvalue=4.4716572949495604e-72)\n",
      "RandomForestRegressor(max_samples=0.2)\n",
      "Fitting Model Now\n",
      "Predicting with Model Now\n",
      "r2 Score: -0.012578233069694189\n",
      "Normal Correlation: 0.01375426078624618\n",
      "(Numerai Score): SpearmanrResult(correlation=0.01556704636230532, pvalue=1.5381865970775864e-09)\n",
      "SGDRegressor(penalty=None)\n",
      "Fitting Model Now\n",
      "Predicting with Model Now\n",
      "r2 Score: -0.005549766966034264\n",
      "Normal Correlation: 0.030727468557724763\n",
      "(Numerai Score): SpearmanrResult(correlation=0.02867862872988659, pvalue=9.017251925377317e-29)\n"
     ]
    }
   ],
   "source": [
    "quick_model(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The newly implemented adaboost did not perform impressively in either r2 Score or correlation, but it did provide a larger p-value than the rest of the models. Because a larger p-value indicates more confidence in predictions, this means that we should retain the idea of using some sort of gradient boosting for our final choice.\n",
    "* The XGBRegressor produced a strong r2 score , although it was negative. This is something that could be corrected through enhanced parameters. The p-value associated with this regressor was also notable when compared ot the performance of a regular gradient boosting regressor. Our third model was a gradient boosting regressor and it had a lower r2 score, although it had a slightly higher correlation,the p-value was essentially half as small as that of the xgbregressor.\n",
    "* Our Random forest regressor with added sampling again performed notably well. The r2 score had the strongest signal of all models (again, it performed worse than random, but we can correct this.) Overall, when considering the scoring of this model, it still shows to be a promising predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above results, we can conclude two things. First, is that Random Forest Regression shows to be the most promising model which can be modified to produce good predictions. Second, is that whenever the concept of boosting is applied to an algorithm, the results tend to score better than without boosting. For the final model selection, we will work to combine the efficiency of XGBoost with the predictive power of the random forest regressor. We will also create a distribution of parameters to conduct cross-validation, to ensure that we get the best performing model possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until this point, I've restricted myself to only documentation from the most popular libraries as well as knowledge from textbooks for potential modeling solutions. This is good practice for fundamentals in ml development and data science, but, to perform as best as possible, I will begin to utilize the most important learning resource for this competition; the community. Numerai's community is an entirely open-source community of data scientists who collaborate, share results, and help each other improve upon ideas to help gain more impressive results. Numerai itsself also provides a comprehensive amount of example material and directions to help newcomers get started with the competition.\n",
    "\n",
    " â€œIf I have seen further it is by standing on the shoulders of Giantsâ€ - Isaac Newton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the best model we can!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "params = [{'max_depth': [5], 'learning_rate': [0.01, 0.05, 0.009],\n",
    "          'n_estimators':[1000, 2000, 3000], 'n_jobs':[-1],\n",
    "          'colsample_bytree': [0.1, 0.15]}]\n",
    "\n",
    "model = XGBRegressor()\n",
    "\n",
    "rcv = RandomizedSearchCV(model, params, cv=5,\n",
    "                         return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=XGBRegressor(base_score=None, booster=None,\n",
       "                                          colsample_bylevel=None,\n",
       "                                          colsample_bynode=None,\n",
       "                                          colsample_bytree=None,\n",
       "                                          enable_categorical=False, gamma=None,\n",
       "                                          gpu_id=None, importance_type=None,\n",
       "                                          interaction_constraints=None,\n",
       "                                          learning_rate=None,\n",
       "                                          max_delta_step=None, max_depth=None,\n",
       "                                          min_child_weight=None, missing=nan,\n",
       "                                          monotone_constraints=...\n",
       "                                          num_parallel_tree=None,\n",
       "                                          predictor=None, random_state=None,\n",
       "                                          reg_alpha=None, reg_lambda=None,\n",
       "                                          scale_pos_weight=None, subsample=None,\n",
       "                                          tree_method=None,\n",
       "                                          validate_parameters=None,\n",
       "                                          verbosity=None),\n",
       "                   param_distributions=[{'colsample_bytree': [0.1, 0.15],\n",
       "                                         'learning_rate': [0.01, 0.05, 0.009],\n",
       "                                         'max_depth': [5],\n",
       "                                         'n_estimators': [1000, 2000, 3000],\n",
       "                                         'n_jobs': [-1]}],\n",
       "                   return_train_score=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.15,\n",
       "             enable_categorical=False, gamma=0, gpu_id=-1, importance_type=None,\n",
       "             interaction_constraints='', learning_rate=0.009, max_delta_step=0,\n",
       "             max_depth=5, min_child_weight=1, missing=nan,\n",
       "             monotone_constraints='()', n_estimators=2000, n_jobs=-1,\n",
       "             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0030261018845151753"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 Score: 0.0032868533081097384\n",
      "Normal Correlation: 0.05748748034409029\n",
      "(Numerai Score): SpearmanrResult(correlation=0.0564940101137895, pvalue=1.154080173305251e-106)\n"
     ]
    }
   ],
   "source": [
    "preds = rcv.predict(X_test)\n",
    "score = r2_score(y_test, preds)\n",
    "print(f\"r2 Score: {score}\")\n",
    "print(f\"Normal Correlation: {correlation_score(y_test, preds)}\")\n",
    "print(f\"(Numerai Score): {numerai_score(y_test, preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:24:36] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1643227205751/work/src/learner.cc:576: \n",
      "Parameters: { \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "r2 Score: -0.005219588971536959\n",
      "Normal Correlation: 0.048749748742907204\n",
      "(Numerai Score): SpearmanrResult(correlation=0.04751967508655768, pvalue=5.4270491092430485e-76)\n"
     ]
    }
   ],
   "source": [
    "model = XGBRegressor(max_depth=5, learning_rate=0.01, n_estimators=20000,\n",
    "                     num_leaves=2**5, n_jobs=-1, colsample_bytree=0.1)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "score = r2_score(y_test, preds)\n",
    "print(f\"r2 Score: {score}\")\n",
    "print(f\"Normal Correlation: {correlation_score(y_test, preds)}\")\n",
    "print(f\"(Numerai Score): {numerai_score(y_test, preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above model did not have as impressive results as desired. It just goes to show how difficult this competition actually is. Regardless, we will not give up! Next, we will try some unsupervised methods for clustering the data, to see if we have better luck at predicting values when using a subset of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.15,\n",
       "             enable_categorical=False, gamma=0, gpu_id=-1, importance_type=None,\n",
       "             interaction_constraints='', learning_rate=0.009, max_delta_step=0,\n",
       "             max_depth=5, min_child_weight=1, missing=nan,\n",
       "             monotone_constraints='()', n_estimators=2000, n_jobs=-1,\n",
       "             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgb.pkl']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(rcv.best_estimator_, 'xgb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:26:59] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:1040: \n",
      "  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n",
      "  older XGBoost, please export the model by calling `Booster.save_model` from that version\n",
      "  first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.\n",
      "\n",
      "[11:26:59] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:749: Found JSON model saved before XGBoost 1.6, please save the model using current version again. The support for old JSON model will be discontinued in XGBoost 2.3.\n",
      "[11:26:59] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-10.9-x86_64-3.7/xgboost/src/learner.cc:438: \n",
      "  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n",
      "  older XGBoost, please export the model by calling `Booster.save_model` from that version\n",
      "  first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mod = joblib.load('xgb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mod.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_plot(importances, X_train, y_train,\n",
    "                 num_of_importances):\n",
    "    \"\"\"\n",
    "    plots feature importances for a given estimator\n",
    "    \n",
    "    args:\n",
    "    importances(array) - array of feature importances given\n",
    "    from estimator attributes\n",
    "    X_train(array) - the training dataset features used to \n",
    "    train our estimator\n",
    "    y_train(array) - the training data labels used to train\n",
    "    our model\n",
    "    num_of_importances(int) - number of features we are \n",
    "    interested in observing importance\n",
    "    \n",
    "    returns: a bar chart of feature importances for a given\n",
    "    ML model\n",
    "    \n",
    "    \"\"\"\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    columns = X_train.columns.values[indices[:num_of_importances]]\n",
    "    values = importances[indices][:num_of_importances]\n",
    "    fig = plt.figure(figsize=(9,5))\n",
    "    plt.title(f\"Weights for top {num_of_importances} features\")\n",
    "    plt.bar(np.arange(num_of_importances),\n",
    "           values, width = 0.6, align=\"center\",\n",
    "           color = '#00A000', label = \"Feature Weight\")\n",
    "    plt.bar(np.arange(num_of_importances) - 0.3,\n",
    "           np.cumsum(values), width = 0.2, align = \"center\",\n",
    "           color = '#00A0A0', \n",
    "           label = \"Cumulative Feature Weight\")\n",
    "    plt.xticks(np.arange(num_of_importances), columns,\n",
    "              rotation=90)\n",
    "    plt.xlim((-0.5, num_of_importances-.05))\n",
    "    plt.ylabel(\"Weight\", fontsize = 12)\n",
    "    plt.xlabel(\"Feature\", fontsize = 12)\n",
    "    \n",
    "    plt.legend(loc = 'upper center')\n",
    "    plt.tight_layout()\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f_imp = mod.feature_importances_\n",
    "feature_plot(f_imp, X_train, y_train, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00313338, 0.003312  , 0.00315049, 0.00287384, 0.00302478,\n",
       "       0.0030771 , 0.00313404, 0.00326649, 0.00245191, 0.00334549,\n",
       "       0.0031599 , 0.00266864, 0.00328334, 0.00315007, 0.00301987,\n",
       "       0.00298859, 0.00321378, 0.00355619, 0.00301725, 0.00318783,\n",
       "       0.00353721, 0.00341381, 0.00326749, 0.00324669, 0.00314776,\n",
       "       0.00294193, 0.00303392, 0.00311576, 0.00257775, 0.00414886,\n",
       "       0.0033823 , 0.00252752, 0.00355981, 0.00306025, 0.00331152,\n",
       "       0.0031755 , 0.00301493, 0.00347665, 0.003042  , 0.00316942,\n",
       "       0.00326219, 0.00303023, 0.0023917 , 0.00297978, 0.00314554,\n",
       "       0.00338003, 0.00333388, 0.00322004, 0.00370787, 0.002945  ,\n",
       "       0.00315098, 0.00310203, 0.00310153, 0.00308957, 0.00312173,\n",
       "       0.00320037, 0.0027941 , 0.0035142 , 0.00302808, 0.00310624,\n",
       "       0.00300901, 0.003205  , 0.00312008, 0.00315456, 0.00308716,\n",
       "       0.00359171, 0.00352346, 0.00313035, 0.00337318, 0.00348314,\n",
       "       0.00282556, 0.00319433, 0.00331421, 0.00310262, 0.00395121,\n",
       "       0.00291575, 0.00298483, 0.00365485, 0.00394029, 0.00310988,\n",
       "       0.00410923, 0.00331777, 0.00332293, 0.0031448 , 0.00303527,\n",
       "       0.00311853, 0.0033008 , 0.00422786, 0.00302556, 0.00372234,\n",
       "       0.00313288, 0.00329546, 0.00387135, 0.00318392, 0.00325557,\n",
       "       0.00311732, 0.00390597, 0.00326597, 0.0032484 , 0.0028863 ,\n",
       "       0.00320621, 0.00359467, 0.00340394, 0.00285992, 0.00321331,\n",
       "       0.00309547, 0.0034188 , 0.00338215, 0.00318028, 0.00332553,\n",
       "       0.00282157, 0.00446657, 0.00304909, 0.00297908, 0.00281056,\n",
       "       0.00305157, 0.00332087, 0.00327266, 0.00322972, 0.00328803,\n",
       "       0.00323039, 0.00289972, 0.00317639, 0.00313662, 0.00301835,\n",
       "       0.00310526, 0.00326318, 0.00340504, 0.00340483, 0.00319887,\n",
       "       0.00310271, 0.00335229, 0.00326785, 0.00319886, 0.00318866,\n",
       "       0.00323407, 0.00309906, 0.00326742, 0.00368421, 0.00389498,\n",
       "       0.00365289, 0.00263081, 0.00427645, 0.003548  , 0.00309644,\n",
       "       0.0032784 , 0.00354502, 0.00385112, 0.0033695 , 0.00436542,\n",
       "       0.00357964, 0.00313149, 0.00309882, 0.00299011, 0.00308091,\n",
       "       0.00323665, 0.00340027, 0.00340711, 0.0031961 , 0.00302932,\n",
       "       0.00321017, 0.00362368, 0.00326652, 0.00320888, 0.00309008,\n",
       "       0.003307  , 0.00325852, 0.00340517, 0.00306959, 0.00299597,\n",
       "       0.00294688, 0.0031341 , 0.00293065, 0.00302785, 0.00343651,\n",
       "       0.00364208, 0.00309161, 0.00307038, 0.00333132, 0.00320965,\n",
       "       0.00313203, 0.00295421, 0.00316128, 0.00307369, 0.00331198,\n",
       "       0.00311945, 0.00317045, 0.00334428, 0.00302978, 0.00330402,\n",
       "       0.00324685, 0.00326104, 0.0033745 , 0.00322263, 0.00289329,\n",
       "       0.00305552, 0.00316412, 0.0031759 , 0.00309493, 0.00357296,\n",
       "       0.00297803, 0.00329652, 0.00314108, 0.00307744, 0.00362958,\n",
       "       0.00342024, 0.00323068, 0.00262604, 0.00317342, 0.00293862,\n",
       "       0.00315953, 0.00312439, 0.00274204, 0.00341026, 0.00290373,\n",
       "       0.00314806, 0.00319595, 0.00333267, 0.00317519, 0.00278935,\n",
       "       0.00307856, 0.00324278, 0.00325929, 0.00328144, 0.00319371,\n",
       "       0.00339441, 0.00330076, 0.00311325, 0.00311534, 0.00343595,\n",
       "       0.00349041, 0.00320704, 0.00338185, 0.00336642, 0.00333824,\n",
       "       0.003096  , 0.00317459, 0.00337266, 0.00325566, 0.00288722,\n",
       "       0.00314613, 0.00338667, 0.00318099, 0.00326722, 0.00320815,\n",
       "       0.00279257, 0.00302714, 0.00329583, 0.00314853, 0.00314955,\n",
       "       0.00326374, 0.00312689, 0.0032204 , 0.00342514, 0.00326856,\n",
       "       0.00323846, 0.00292187, 0.00288606, 0.00289763, 0.00339801,\n",
       "       0.00343548, 0.00315247, 0.00287497, 0.00331551, 0.00304575,\n",
       "       0.00330801, 0.00265612, 0.00314615, 0.00339684, 0.00309034,\n",
       "       0.00324164, 0.00336248, 0.00323151, 0.00300708, 0.00322141,\n",
       "       0.00334335, 0.00301686, 0.00332875, 0.0031938 , 0.00311349,\n",
       "       0.00316408, 0.00331216, 0.00307777, 0.0033601 , 0.00384475,\n",
       "       0.00339316, 0.00359469, 0.00315675, 0.00350052, 0.00321351,\n",
       "       0.00318721, 0.00318935, 0.00320246, 0.00324117, 0.00314137,\n",
       "       0.00372657, 0.00324471, 0.00348782, 0.00261377, 0.00310511,\n",
       "       0.00316046, 0.00329954, 0.00358421, 0.0030579 , 0.00358392,\n",
       "       0.00320342, 0.00307187, 0.00312914, 0.00309271, 0.00313655],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:numerai]",
   "language": "python",
   "name": "conda-env-numerai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
